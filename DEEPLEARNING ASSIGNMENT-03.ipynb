{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6559df53",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbce30b",
   "metadata": {},
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b934dd23",
   "metadata": {},
   "source": [
    "Initializing all weights to the same value, even if it is randomly selected using He initialization, is not recommended as it leads to symmetry breaking, where all neurons in a layer will end up learning the same feature, and the network will fail to learn diverse representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f6865",
   "metadata": {},
   "source": [
    "2. Is it OK to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eba96",
   "metadata": {},
   "source": [
    "It is generally fine to initialize bias terms to 0, as they will be learned during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccd3e7",
   "metadata": {},
   "source": [
    "\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e2fd26a",
   "metadata": {},
   "source": [
    "The three advantages of the SELU activation function over ReLU are as follows:\n",
    "\n",
    "It avoids the dying ReLU problem by allowing negative values, which can lead to faster and more stable convergence.\n",
    "It is self-normalizing, which means it helps preserve the mean and variance of the input data, making it well-suited for deep neural networks.\n",
    "It can achieve state-of-the-art performance on many types of neural network architectures and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4110a",
   "metadata": {},
   "source": [
    "\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f60a657b",
   "metadata": {},
   "source": [
    "The activation functions should be chosen based on the type of data and the task at hand:\n",
    "SELU should be used in deep neural networks with dense architectures.\n",
    "Leaky ReLU and its variants should be used when there is a risk of dying ReLU and sparse representations are desired.\n",
    "ReLU should be used as a default activation function for most neural networks.\n",
    "Tanh and logistic should be used when the output range needs to be between -1 and 1 or 0 and 1, respectively.\n",
    "Softmax should be used in classification tasks where multiple classes are present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8389a0",
   "metadata": {},
   "source": [
    "\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c02c44",
   "metadata": {},
   "source": [
    "Setting the momentum hyperparameter too close to 1 can lead to the optimizer overshooting the minimum and oscillating around it, resulting in slower convergence and poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e77ab",
   "metadata": {},
   "source": [
    "\n",
    "6. Name three ways you can produce a sparse model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd167b9",
   "metadata": {},
   "source": [
    "Three ways to produce a sparse model are:\n",
    "\n",
    "L1 regularization, which encourages some of the weights to be set to zero.\n",
    "Dropout, which randomly drops out some neurons during training, forcing the network to learn more robust and diverse features.\n",
    "Batch normalization, which can make the network more sparse by reducing the covariance shift between layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915672a",
   "metadata": {},
   "source": [
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3b71e9e",
   "metadata": {},
   "source": [
    "Dropout can slow down training as it requires more iterations to converge, but it helps prevent overfitting and leads to better generalization. It does not slow down inference, as the neurons are no longer dropped out. MC Dropout can slow down inference as it requires multiple forward passes to estimate the uncertainty, but it can also provide more accurate predictions and uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d7dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
