{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36faacc1",
   "metadata": {},
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67638df",
   "metadata": {},
   "source": [
    "a) Sigmoid is an activation function that maps any input to a value between 0 and 1. It is used for binary classification problems.            \n",
    "b) Tanh is an activation function that maps any input to a value between -1 and 1. It is used for binary classification problems.                     \n",
    "c) ReLU (Rectified Linear Unit) is an activation function that returns the input if it is positive, and zero otherwise. It is used for classification problems.                         \n",
    "d) ELU (Exponential Linear Unit) uses a log curve to define negative values unlike the leaky ReLU. It produces negative outputs, which makes it useful for regression problems.                                \n",
    "e) LeakyReLU (Leaky Rectified Linear Unit) returns the input if it is positive, and a small fraction of the input otherwise. It can be used in computer vision, speech recognition, and natural language processing.            \n",
    "f) Swish multiplies the input by the sigmoid of the input. It has been shown to perform better than ReLU on some tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e38448",
   "metadata": {},
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44271d4b",
   "metadata": {},
   "source": [
    "Increasing the optimizer learning rate can cause the model to converge too quickly to a suboptimal solution, whereas decreasing the optimizer learning rate can cause training to progress very slowl. The learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect to the loss gradient. A lower learning rate will result in slower but more accurate convergence, while a higher learning rate will result in faster but less accurate convergence. However, increasing the learning rate during training (and then decreasing it again) can lead you to lower values of your loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986999c",
   "metadata": {},
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b49eb4",
   "metadata": {},
   "source": [
    "Increasing the number of internal hidden neurons can improve the accuracy of a neural network, but it can also result in overfitting if there are too many neurons. The effect of increasing the number of hidden layers and neurons on model performance depends on the complexity of the problem being solved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a646cc1",
   "metadata": {},
   "source": [
    "4. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d677c4b",
   "metadata": {},
   "source": [
    "Increasing the size of batch computation can slow down the learning process, but it can also result in a convergence to a more stable model exemplified by lower generalization error. However, according to popular knowledge, increasing batch size reduces the learners' capacity to generalize. Practitioners often want to use a larger batch size because it can lead to faster training times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac7ef1",
   "metadata": {},
   "source": [
    "5. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dfe746",
   "metadata": {},
   "source": [
    "We adopt regularization to avoid overfitting because it removes extra weights from the selected features and redistributes the weights evenly, which discourages the model from relying too much on any one feature. Regularization attempts to reduce the variance of the estimator by simplifying it, something that will increase the bias, in such a way that the model generalizes better to new data. L1 regularization combats overfitting by shrinking the parameters towards 0, which makes some features completely irrelevant for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e6801",
   "metadata": {},
   "source": [
    "6. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e3c84",
   "metadata": {},
   "source": [
    "In deep learning, the loss function is a function that measures how well the model is performing on the training data. The cost function is a function that measures how well the model is performing on both the training and validation data. The goal of training a deep learning model is to minimize the loss or cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97367e9b",
   "metadata": {},
   "source": [
    "7. What do ou mean by underfitting in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3df68",
   "metadata": {},
   "source": [
    "Underfitting in neural networks occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularizatioN. The best methods of dealing with an underfitting model are trying a bigger neural network (adding new layers or increasing the number of neurons per layer), changing the activation function from the logistic sigmoid to another function such as ReLU, and reducing regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3747d089",
   "metadata": {},
   "source": [
    "8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e7d8b",
   "metadata": {},
   "source": [
    "We use dropout in neural networks to prevent overfitting by simulating a sparse activation from a given layer, which encourages the network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. Dropout also helps in shrinking the squared norm of the weights, which tends to a reduction in overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d3222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
