{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca6ffad",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860df577",
   "metadata": {},
   "source": [
    "1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What\n",
    "are its main components?\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47de2722",
   "metadata": {},
   "source": [
    "An artificial neuron, also known as a perceptron, is a fundamental building block of artificial neural networks. It is inspired by the biological neuron, which is the basic unit of the human nervous system. The structure of an artificial neuron consists of three main components: inputs, weights, and a bias. The inputs are the signals that are received by the neuron, which can be either real values or binary values. The weights are the coefficients that are assigned to each input, which determine the strength of the connection between the input and the neuron. The bias is an additional input to the neuron that is used to adjust the output of the neuron. The output of the neuron is calculated by taking the weighted sum of the inputs and bias, and passing it through an activation function. The activation function is used to introduce non-linearity to the output of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91087db5",
   "metadata": {},
   "source": [
    "2. What are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "raw",
   "id": "76853d35",
   "metadata": {},
   "source": [
    "Sigmoid function: The sigmoid function is a popular activation function that is used in binary classification tasks. It maps the input to a value between 0 and 1, which can be interpreted as the probability of the input belonging to the positive class. The formula for the sigmoid function is: f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "ReLU function: The ReLU (Rectified Linear Unit) function is a widely used activation function that is used in deep learning networks. It is a simple and efficient function that is computationally cheap. The ReLU function returns the input if it is positive, and 0 otherwise. The formula for the ReLU function is: f(x) = max(0, x)\n",
    "\n",
    "Tanh function: The tanh function is a popular activation function that is used in regression tasks. It maps the input to a value between -1 and 1. The tanh function is similar to the sigmoid function but produces output values that are centered around 0. The formula for the tanh function is: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "Softmax function: The softmax function is a popular activation function that is used in multi-class classification tasks. It maps the input to a probability distribution over the output classes. The softmax function ensures that the sum of the output probabilities is equal to 1. The formula for the softmax function is: f(x_i) = exp(x_i) / sum(exp(x_j)) for all j.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccab7b",
   "metadata": {},
   "source": [
    "3. What is artificial neural network (ANN)? Explain some of the salient highlights in the\n",
    "different architectural options for ANN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f29d9f4",
   "metadata": {},
   "source": [
    "An artificial neural network (ANN) is a computational model inspired by the structure and function of the human brain. It is composed of a large number of interconnected processing elements called neurons, which are organized in layers to process and transform input data into meaningful output. Some salient highlights of different architectural options for ANN are:\n",
    "Single-layer feedforward network: consists of one input layer, one output layer, and no hidden layers. It is used for simple classification problems with linearly separable input data.\n",
    "\n",
    "Multi-layer feedforward network: consists of one input layer, one output layer, and one or more hidden layers. It is used for more complex classification problems with non-linearly separable input data.\n",
    "\n",
    "Recurrent network: consists of one or more feedback loops, allowing the network to use past output as input. It is used for time-series prediction, language modeling, and other sequential tasks. \n",
    "\n",
    "Convolutional network: consists of convolutional layers that extract local features from input data, followed by fully connected layers that perform classification. It is used for image and speech recognition tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0d023",
   "metadata": {},
   "source": [
    "\n",
    "4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning\n",
    "synaptic weights for the interconnection between neurons? How can this challenge be\n",
    "addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891a26c",
   "metadata": {},
   "source": [
    "The learning process of an ANN involves adjusting the weights of the connections between neurons to minimize the difference between the predicted output and the desired output. This is typically done using a training dataset with known input-output pairs. The challenge in assigning synaptic weights for the interconnection between neurons is to find the optimal weights that can achieve the desired output with minimum error. This challenge can be addressed by using various optimization techniques such as gradient descent, which adjusts the weights iteratively to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e94add",
   "metadata": {},
   "source": [
    "\n",
    "5. Explain, in details, the backpropagation algorithm. What are the limitations of this\n",
    "algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6235e6e",
   "metadata": {},
   "source": [
    "The backpropagation algorithm is a popular method for training multi-layer feedforward neural networks. It involves computing the error between the predicted output and the desired output, and then propagating it backward through the network to adjust the weights of the connections between neurons. The algorithm uses the chain rule of calculus to compute the gradient of the error with respect to each weight, and then updates the weights in the direction of the negative gradient to minimize the error. The limitations of this algorithm include the potential for getting stuck in local minima, slow convergence, and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd923f",
   "metadata": {},
   "source": [
    "\n",
    "6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer\n",
    "neural network.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ba06de5",
   "metadata": {},
   "source": [
    "The process of adjusting the interconnection weights in a multi-layer neural network involves forward propagation of the input through the network to compute the output, and then backward propagation of the error to adjust the weights using gradient descent. This process is repeated iteratively until the error is minimized. The adjustment of weights is based on the amount of error propagated backwards from the output layer to the hidden layers, and then to the input layer. The weights are updated by multiplying the error by the activation of the neuron and the learning rate, and then subtracting the result from the current weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263197b",
   "metadata": {},
   "source": [
    "\n",
    "7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is\n",
    "required?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "509918d4",
   "metadata": {},
   "source": [
    "The steps in the backpropagation algorithm are:\n",
    "\n",
    "Initialize the weights randomly.\n",
    "Forward propagate the input through the network to compute the output.\n",
    "Compute the error between the predicted output and the desired output.\n",
    "Backward propagate the error through the network to adjust the weights using gradient descent.\n",
    "Repeat steps 2-4 for a number of epochs or until the error is minimized.\n",
    "A multi-layer neural network is required to model non-linear relationships between input and output data. A single-layer network can only model linear relationships, which limits its ability to solve complex problems. By adding hidden layers to the network, it becomes possible to model non-linear relationships and learn more complex features in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7247f",
   "metadata": {},
   "source": [
    "8.Write short notes on:\n",
    "\n",
    "1. Artificial neuron\n",
    "2. Multi-layer perceptron\n",
    "3. Deep learning\n",
    "4. Learning rate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92ae578d",
   "metadata": {},
   "source": [
    "Artificial neuron: An artificial neuron, also called a perceptron, is a fundamental unit of an artificial neural network (ANN). It is inspired by the biological neuron and performs computations on its inputs, producing an output signal. An artificial neuron consists of inputs, weights, a bias term, and an activation function. The inputs are multiplied by their respective weights and the products are summed up along with the bias term. The sum is then passed through the activation function to produce the output signal. The activation function introduces nonlinearity into the system and helps in modeling complex relationships between inputs and outputs.\n",
    "\n",
    "Multi-layer perceptron: A multi-layer perceptron (MLP) is a feedforward neural network that contains multiple layers of perceptrons. The first layer takes the input signal and passes it to the next layer. Each subsequent layer takes the output of the previous layer as input and applies its own weights and biases to produce its own output. The final output is obtained from the output layer. MLPs are capable of approximating any continuous function, and they have been successfully applied to a wide range of applications such as image and speech recognition, natural language processing, and financial forecasting.\n",
    "\n",
    "Deep learning: Deep learning is a subfield of machine learning that involves the use of deep neural networks with multiple layers. Deep learning algorithms learn from large amounts of data and can automatically extract features that are relevant for the task at hand. Deep learning has been successfully applied to a wide range of applications such as image and speech recognition, natural language processing, and game playing.\n",
    "\n",
    "Learning rate: Learning rate is a hyperparameter that determines the step size at which an algorithm moves towards a minimum of the loss function during training. The learning rate determines how quickly the algorithm converges to a solution and also affects the quality of the solution. If the learning rate is too high, the algorithm may overshoot the minimum and diverge. If the learning rate is too low, the algorithm may take too long to converge or get stuck in a local minimum. The learning rate is a critical hyperparameter that needs to be tuned carefully to achieve good performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
