{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf17bd6",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e536a",
   "metadata": {},
   "source": [
    "1. What are the pros and cons of using a stateful RNN versus a stateless RNN?\n",
    "2. Why do people use Encoderâ€“Decoder RNNs rather than plain sequence-to-sequence RNNs\n",
    "for automatic translation?\n",
    "3. How can you deal with variable-length input sequences? What about variable-length output\n",
    "sequences?\n",
    "4. What is beam search and why would you use it? What tool can you use to implement it?\n",
    "5. What is an attention mechanism? How does it help?\n",
    "6. What is the most important layer in the Transformer architecture? What is its purpose?\n",
    "7. When would you need to use sampled softmax?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "070add2f",
   "metadata": {},
   "source": [
    "1A) \n",
    "  A stateful RNN has the advantage of being able to maintain context across batches, making it useful for tasks such as text generation, where the previous words heavily influence the next word. However, it requires careful management of state initialization and resetting, and can be slower to train than a stateless RNN, which does not carry any context across batches.\n",
    "\n",
    "2A)\n",
    "Encoder-Decoder RNNs are better suited for automatic translation than plain sequence-to-sequence RNNs because they can handle variable-length input and output sequences. The encoder network reads in the input sequence and compresses it into a fixed-length vector, which is then fed to the decoder network to generate the output sequence. This allows the model to handle input and output sequences of different lengths.\n",
    "\n",
    "3A)\n",
    "Variable-length input sequences can be handled by padding the sequences to a fixed length, or by using a masking mechanism that allows the model to ignore padded elements. Variable-length output sequences can be handled by using a dynamic decoding approach that generates outputs one at a time, using the previously generated output as input to the next step.\n",
    "\n",
    "4A)\n",
    "Beam search is a search algorithm used in natural language processing and speech recognition to find the most likely sequence of words. It works by maintaining a beam of the most likely partial hypotheses, and expanding the beam at each step to consider multiple possible next words. Beam search can improve the accuracy of sequence generation but can be computationally expensive. A common tool used to implement beam search is the OpenNMT library.\n",
    "\n",
    "5A)\n",
    "An attention mechanism is a component of a neural network that allows the model to focus on different parts of the input sequence when making predictions. It works by assigning weights to each input element based on its relevance to the current prediction, and then computing a weighted sum of the input elements. This allows the model to selectively attend to the most important parts of the input sequence, improving the accuracy of the predictions.\n",
    "\n",
    "6A)\n",
    "The most important layer in the Transformer architecture is the self-attention layer, which computes the attention weights between all pairs of input elements. It allows the model to attend to all input elements simultaneously, rather than sequentially like traditional RNNs, making it more efficient for handling long input sequences.\n",
    "\n",
    "7A)\n",
    "Sampled softmax is used when the number of output classes is very large, making it computationally expensive to compute the softmax probabilities for all possible classes. It works by randomly sampling a subset of classes and computing the softmax probabilities only for those classes. This can greatly reduce the computation time required to compute the output probabilities, at the cost of some reduction in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566a1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
