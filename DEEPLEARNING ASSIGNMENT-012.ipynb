{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4237cf",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ef70f",
   "metadata": {},
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n",
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "6. Implement matmul using Einstein summation.\n",
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dbb7bf5",
   "metadata": {},
   "source": [
    "1A)\n",
    "unsqueeze helps us to solve broadcasting problems by adding an extra dimension to a tensor, effectively increasing its rank. This allows the tensor to match the shape of another tensor for broadcasting operations.\n",
    "\n",
    "2A)\n",
    "We can use indexing to achieve the same operation as unsqueeze by using the None keyword in the index tuple. For example, tensor[:, None] would add a new dimension to the tensor.\n",
    "\n",
    "3A)\n",
    "To show the actual contents of the memory used for a tensor in PyTorch, we can use the tensor.data_ptr() method. This returns a pointer to the start of the tensor's data in memory, which can then be used with other tools such as a memory debugger to inspect the memory contents.\n",
    "\n",
    "4A)\n",
    "When adding a vector of size 3 to a matrix of size 3×3, the elements of the vector are added to each column of the matrix. This is because PyTorch broadcasting rules align the dimensions of the vector (1D) and the matrix (2D) along the last dimension of the matrix.\n",
    "\n",
    "5A)\n",
    "Broadcasting and expand_as do not necessarily result in increased memory use, as they can often perform operations on existing memory rather than creating new memory. However, in some cases, such as when broadcasting a large tensor to match the shape of a much smaller tensor, the resulting broadcasted tensor may use more memory than either of the original tensors.\n",
    "\n",
    "6A)\n",
    "We can implement matmul using Einstein summation as follows: torch.einsum('ij,jk->ik', tensor1, tensor2). This computes the matrix multiplication of tensor1 and tensor2.\n",
    "\n",
    "7A)\n",
    "A repeated index letter on the lefthand side of einsum represents a summation over that index. For example, in the expression torch.einsum('ii->i', tensor), the repeated index i indicates that we want to sum over the first and second dimensions of tensor and return a tensor with shape (n,).\n",
    "\n",
    "8A)\n",
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "Repeated indices are implicitly summed over.\n",
    "Each index can appear at most twice in the expression, once as an input and once as an output.\n",
    "The order of the input and output indices determines the order of the dimensions in the output tensor.\n",
    "These rules are designed to provide a concise and flexible way of expressing tensor operations.\n",
    "\n",
    "9A)\n",
    "The forward pass of a neural network refers to the process of applying the network's weights and biases to its inputs to generate a prediction. The backward pass, also known as backpropagation, refers to the process of computing the gradients of the network's weights and biases with respect to a loss function, which allows the weights and biases to be updated using an optimization algorithm such as gradient descent.\n",
    "\n",
    "10)\n",
    "Storing activations calculated for intermediate layers in the forward pass is necessary for backpropagation. During backpropagation, the gradients are calculated starting from the output of the network and working backwards through the layers. In order to calculate the gradient of a given layer, we need the activations from that layer in the forward pass.\n",
    "\n",
    "11)\n",
    "The downside of having activations with a standard deviation too far away from 1 is that it can lead to either vanishing or exploding gradients. Vanishing gradients occur when the gradient of the loss with respect to the weights becomes very small, making it difficult for the optimizer to update the weights effectively. Exploding gradients occur when the gradient becomes very large, causing the weights to be updated too aggressively and leading to instability in the training process.\n",
    "\n",
    "12)\n",
    "Weight initialization can help avoid the problem of activations with a standard deviation too far away from 1 by ensuring that the weights are initialized in a way that balances the variance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
