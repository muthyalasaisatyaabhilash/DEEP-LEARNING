{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cba3de",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e7a89",
   "metadata": {},
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "3. Name three popular activation functions. Can you draw them?\n",
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    " What is the shape of the input matrix X?\n",
    " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    " What is the shape of the network’s output matrix Y?\n",
    " Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n",
    "\n",
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n",
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n",
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e8b1642",
   "metadata": {},
   "source": [
    "1A)Logistic Regression is generally preferred over the Perceptron because it can output class probabilities, while the Perceptron can only make binary classifications. Moreover, Logistic Regression is a more robust model than the Perceptron, and it can handle linearly inseparable data. To make a Perceptron equivalent to a Logistic Regression classifier, we can use the logistic activation function instead of the step function in the output layer of the Perceptron.\n",
    "\n",
    "2A)The logistic activation function was a key ingredient in training the first MLPs because it is a differentiable function that allows for gradient descent optimization during backpropagation. The logistic function is also helpful because it squashes its input into the range of [0,1], allowing the output of each neuron to be interpreted as a probability.\n",
    "\n",
    "3A)Three popular activation functions are:\n",
    "\n",
    "ReLU (Rectified Linear Unit): f(x) = max(0, x)\n",
    "sigmoid: f(x) = 1 / (1 + exp(-x))\n",
    "tanh (hyperbolic tangent): f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "4A)\n",
    "The shape of the input matrix X is (m, 10), where m is the number of instances in the dataset.\n",
    "The shape of the hidden layer's weight vector Wh is (10, 50), and the shape of its bias vector bh is (50,).\n",
    "The shape of the output layer's weight vector Wo is (50, 3), and its bias vector bo is (3,).\n",
    "The shape of the network's output matrix Y is (m, 3).\n",
    "The equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, and bo is: Y = ReLU(X.dot(Wh) + bh).dot(Wo) + bo\n",
    "\n",
    "5A)\n",
    "If you want to classify email into spam or ham, you need 1 neuron in the output layer with the sigmoid activation function. If you want to tackle MNIST, you need 10 neurons in the output layer with the softmax activation function.\n",
    "\n",
    "6A)\n",
    "Backpropagation is an algorithm used for training MLPs that involves computing the gradient of the loss function with respect to the weights and biases of the network, and then adjusting those weights and biases using gradient descent. Backpropagation works by propagating the errors from the output layer back through the network to the input layer, adjusting the weights and biases along the way. Reverse-mode autodiff is a variant of automatic differentiation that is used to compute gradients efficiently, and it is a key component of backpropagation.\n",
    "\n",
    "7A)\n",
    "Some hyperparameters you can tweak in an MLP include the number of hidden layers, the number of neurons in each layer, the activation function used in each layer, the learning rate, the batch size, the number of epochs, the weight initialization method, and the regularization parameters. If an MLP overfits the training data, you can try reducing the number of neurons in the hidden layers, increasing the regularization strength, or using dropout to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e98924",
   "metadata": {},
   "source": [
    "8A)Here's an example code snippet to train a deep MLP on the MNIST dataset using TensorFlow, with the bells and whistles included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_path = \"model_checkpoint.h5\"\n",
    "model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=[model_checkpoint, tensorboard_callback])\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdcfbe3",
   "metadata": {},
   "source": [
    "This code defines a deep MLP with two hidden layers of 512 neurons each, ReLU activation functions, and dropout regularization. It trains the model for 20 epochs using a batch size of 128, and uses the RMSprop optimizer with categorical cross-entropy loss. It also includes a model checkpoint callback to save the best-performing model, and a TensorBoard callback to log training summaries and plot learning curves. With this setup, the model should achieve over 98% precision on the MNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae74e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
