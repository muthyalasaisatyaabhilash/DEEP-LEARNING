{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696f88b1",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d06227e",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "2. Write the Python code to implement ReLU.\n",
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "and functionality built into Python).\n",
    "5. What is the “hidden size” of a layer?\n",
    "6. What does the t method do in PyTorch?\n",
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "8. In matmul, why is ac==br?\n",
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "10. What is elementwise arithmetic?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7678bbb",
   "metadata": {},
   "source": [
    "1A)Here's an example Python code to implement a single neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6b9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = np.dot(inputs, self.weights) + self.bias\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6033272",
   "metadata": {},
   "source": [
    "2A)Here's an example Python code to implement ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66daf152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dc922",
   "metadata": {},
   "source": [
    "3A)Here's an example Python code for a dense layer in terms of matrix multiplication using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee409f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DenseLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.randn(input_size, output_size))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        z = torch.matmul(inputs, self.weights) + self.bias\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66bdffcd",
   "metadata": {},
   "source": [
    "4A)Here's an example Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        z = [sum(x * w) for x, w in zip(inputs, self.weights)]\n",
    "        z = np.array(z) + self.bias\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8dd2ba7",
   "metadata": {},
   "source": [
    "5A)\n",
    "The \"hidden size\" of a layer refers to the number of neurons in that layer. It is called \"hidden\" because it is not part of the input or output layers.\n",
    "\n",
    "6A)\n",
    "The t method in PyTorch is used to transpose a tensor. It returns a new tensor with the dimensions reversed.\n",
    "\n",
    "7A)\n",
    "Matrix multiplication written in plain Python is very slow because it involves nested loops over the elements of the matrices, which is computationally expensive. This can be optimized using libraries like NumPy or PyTorch, which use efficient algorithms and are implemented in C or CUDA.\n",
    "\n",
    "8A)\n",
    "In matrix multiplication, ac==br is a condition that must be satisfied for the matrices to be multiplied. It means that the number of columns of matrix A must be equal to the number of rows of matrix B.\n",
    "\n",
    "9A)\n",
    "In Jupyter Notebook, you can measure the time taken for a single cell to execute using the %timeit magic command. Simply write %timeit before the code you want to time, and Jupyter Notebook will execute the code multiple times and report the average time taken.\n",
    "\n",
    "10)\n",
    "Elementwise arithmetic refers to performing arithmetic operations on each element of an array or tensor individually, rather than on the entire array or tensor at once. For example, adding two arrays elementwise means adding each element of the first array to the corresponding element of the second array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d2665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa177ca7",
   "metadata": {},
   "source": [
    "11. Write the PyTorch code to test whether every element of a is greater than the\n",
    "corresponding element of b.\n",
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "13. How does elementwise arithmetic help us speed up matmul?\n",
    "14. What are the broadcasting rules?\n",
    "15. What is expand_as? Show an example of how it can be used to match the results of\n",
    "broadcasting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "862a16bb",
   "metadata": {},
   "source": [
    "11A)Here's an example PyTorch code to test whether every element of a is greater than the corresponding element of b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "all_greater = torch.all(a > b)\n",
    "print(all_greater)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd37de4",
   "metadata": {},
   "source": [
    "This code creates two tensors, a and b, and uses the > operator to create a Boolean tensor with True where the condition is satisfied and False otherwise. The torch.all function then returns True if all elements of the Boolean tensor are True, and False otherwise."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e31d4040",
   "metadata": {},
   "source": [
    "12A)A rank-0 tensor is a tensor with no dimensions, also known as a scalar. To convert it to a plain Python data type, you can use the .item() method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1)\n",
    "x_scalar = x.item()\n",
    "print(x_scalar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811defc",
   "metadata": {},
   "source": [
    "This code creates a rank-0 tensor x with the value 1, and uses the .item() method to convert it to a plain Python integer."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2000997",
   "metadata": {},
   "source": [
    "13A)\n",
    "Elementwise arithmetic can help us speed up matmul by reducing the number of loops we need to perform. Instead of looping over the elements of the matrices and performing the multiplication and addition operations for each pair of elements, we can use broadcasting to perform the operations on entire rows or columns of the matrices at once.\n",
    "\n",
    "14A)\n",
    "The broadcasting rules define how two tensors with different shapes can be combined elementwise. The rules are:\n",
    "\n",
    "If the two tensors have the same number of dimensions, but one or more of the dimensions have size 1, the tensor with size 1 is broadcasted along that dimension to match the size of the other tensor.\n",
    "If the two tensors have a different number of dimensions, the tensor with fewer dimensions is broadcasted by adding singleton dimensions to the left until the number of dimensions matches."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0fb1092",
   "metadata": {},
   "source": [
    "15A)expand_as is a PyTorch method that can be used to expand the dimensions of a tensor to match the shape of another tensor. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cdb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([10, 20])\n",
    "\n",
    "c = b.unsqueeze(1).expand_as(a)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da90ef4",
   "metadata": {},
   "source": [
    "This code creates two tensors, a and b, and uses the unsqueeze method to add a new dimension to b so that it has shape (2, 1). It then uses the expand_as method to expand b to match the shape of a, which has shape (2, 2). The result is a tensor c with shape (2, 2) that has the same values as b broadcasted along the second dimension to match the shape of a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebbaba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
