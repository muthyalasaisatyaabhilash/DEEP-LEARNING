{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b878ff3",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-014"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c53fc",
   "metadata": {},
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e26506",
   "metadata": {},
   "source": [
    "No, it is not okay to initialize all the weights to the same value even if that value is selected randomly using he initialization. Initializing all weights to the same value can cause each neuron to do the same thing, which reduces the effectiveness of the neural network. Instead, it is recommended to initialize weights with small random values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00bd9e7",
   "metadata": {},
   "source": [
    "2. Is it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e38c56",
   "metadata": {},
   "source": [
    "Yes, it is possible and common to initialize the biases to be zero. The asymmetry breaking is provided by the small random numbers in the weights. However, in general practice, biases are initialized with 0 and weights are initialized with random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fff95e",
   "metadata": {},
   "source": [
    "3. Name three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e388a",
   "metadata": {},
   "source": [
    "Three advantages of the ELU activation function over ReLU are that it produces negative outputs, it is smoother than ReLU, and it leads to lower training times and better generalization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ca183",
   "metadata": {},
   "source": [
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffca9b",
   "metadata": {},
   "source": [
    "ELU is best used for regression problem, Leaky ReLU and its variants are best used for computer vision, speech recognition, and natural language processing, ReLU is best used for classification problem, tanh is best used for binary classification problems, logistic is best used for binary classification problems, and softmax is best used for multi-class classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8ab00",
   "metadata": {},
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9811",
   "metadata": {},
   "source": [
    "If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer, then the algorithm will likely pick up a lot of speed, which can cause it to overshoot the minimum and fail to converg. It is recommended to test momentum values in the range of 0.9 to 0.99 and choose a value that works best for your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6303c4d1",
   "metadata": {},
   "source": [
    "6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86a477",
   "metadata": {},
   "source": [
    "Three ways to produce a sparse model are to remove features from the model, make the features dense, and use models that are robust to sparsity. Sparsity is beneficial because sparse models are more easily interpretable by humans, and sparsity can yield statistical benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b754bd",
   "metadata": {},
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c411e158",
   "metadata": {},
   "source": [
    "Dropout can slow down training because it makes the training process noisy, forcing nodes within a layer to probabilistically take on more or less importance in the model. Dropout does not slow down inference (i.e., making predictions on new instances) because it is only used during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db7130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
