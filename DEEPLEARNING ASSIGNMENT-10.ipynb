{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5efff423",
   "metadata": {},
   "source": [
    "# DEEPLEARNING ASSIGNMENT-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53056c",
   "metadata": {},
   "source": [
    "1. What does a SavedModel contain? How do you inspect its content?\n",
    "2. When should you use TF Serving? What are its main features? What are some tools you can\n",
    "use to deploy it?\n",
    "3. How do you deploy a model across multiple TF Serving instances?\n",
    "4. When should you use the gRPC API rather than the REST API to query a model served by TF\n",
    "Serving?\n",
    "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
    "embedded device?\n",
    "6. What is quantization-aware training, and why would you need it?\n",
    "7. What are model parallelism and data parallelism? Why is the latter\n",
    "generally recommended?\n",
    "8. When training a model across multiple servers, what distribution strategies can you use?\n",
    "How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d201166d",
   "metadata": {},
   "source": [
    "1A)\n",
    "A SavedModel is a serialized format for TensorFlow models that contains all the necessary information to run the model, including the model's graph, variables, and metadata. It also includes information about how to preprocess inputs and postprocess outputs, as well as any other assets the model may depend on. You can inspect its content using the saved_model_cli command-line tool or by loading the SavedModel into a TensorFlow session and examining its contents using standard TensorFlow APIs.\n",
    "\n",
    "2A)\n",
    "You should use TensorFlow Serving (TF Serving) when you want to deploy your trained TensorFlow models to production environments at scale. Its main features include support for multiple models and versions, flexible deployment configurations, and high-performance serving of models with low latency and high throughput. Some tools you can use to deploy TF Serving include Kubernetes, Docker, and Google Cloud Platform.\n",
    "\n",
    "3A)\n",
    "To deploy a model across multiple TF Serving instances, you can use TF Serving's built-in model replication feature. This allows you to replicate a model across multiple instances for improved availability and scalability. You can also use load balancing tools like Kubernetes to distribute incoming requests across multiple instances.\n",
    "\n",
    "4A)\n",
    "You should use the gRPC API rather than the REST API to query a model served by TF Serving when you require low-latency and high-throughput inference, as gRPC provides a more efficient communication protocol for this purpose. However, if you require a simpler, more widely supported API, or if you need to integrate with existing systems that only support REST, then you may choose to use the REST API instead.\n",
    "\n",
    "5A)\n",
    "TFLite reduces a model's size to make it run on mobile or embedded devices using several techniques, including quantization, pruning, and model compression. Quantization reduces the precision of the model's weights and activations, while pruning removes unimportant weights from the model. Model compression techniques, such as Huffman coding or weight sharing, further reduce the size of the model.\n",
    "\n",
    "6A)\n",
    "Quantization-aware training is a technique for training models with reduced precision weights and activations, which can improve model size and performance on hardware with limited computational resources. It involves simulating the effects of quantization during training by adding quantization noise to the weights and activations, which allows the model to learn to be more robust to reduced precision.\n",
    "\n",
    "7A)\n",
    "Model parallelism and data parallelism are two techniques for training large neural networks that cannot fit in a single GPU or machine. Model parallelism involves dividing the model across multiple devices or machines and computing each part separately, while data parallelism involves replicating the model across multiple devices or machines and computing different batches of data in parallel. Data parallelism is generally recommended because it is easier to implement and can scale to larger models and datasets.\n",
    "\n",
    "8A)\n",
    "When training a model across multiple servers, you can use several distribution strategies, including mirrored strategy, parameter server strategy, and collective all-reduce strategy. Mirrored strategy involves replicating the entire model across multiple devices and synchronizing gradients during training. Parameter server strategy involves dividing the model across multiple parameter servers, each responsible for a subset of the model's variables. Collective all-reduce strategy involves dividing the data across multiple devices and aggregating gradients using all-reduce operations. The choice of strategy depends on the size of the model, the available computational resources, and the communication infrastructure.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
